\section{Implementierung}

In dem folgenden Kapitel wird beschrieben, wie die Theorie aus den vorherigen Kapiteln in der Implementierung umgesetzt wurden 
und die Implementierung an sich beschrieben. 
Dabei wird zunächst der oberflächliche Aufbau der Implementierung erklärt und dann auf die einzelnen Bestandteile der Implementierung eingegangen. 
Zur Verdeutlichung werden außerdem noch Besonderheiten aus der erstellten Implementierung aufgezeigt und deren Umsetzung beschrieben. 

\subsection{Aufbau der Implementierung}

Der gesamte Aufbau der Implementierung ist in drei Projekte aufgeteilt: Core, Simulation und \ac{lidar}. 
Das Core-Projekt ist eine Library, welche keine ausführbare Datei und lediglich die Implementierungen der Algorithmen bzw. die Logik 
für das Steuern und Ausweichen des Fahrzeugs enthält. 
Das Simulation-Projekt dient für die Simulation des autonomen Fahrzeugs und zum Testen der implementierten Algorithmen. 
Das \ac{lidar}-Projekt enthält den Code, welcher auf das eigentliche Fahrzeug, 
bzw. den Raspberry Pi des Fahrzeugs, geladen und auf diesem ausgeführt wird. 
Das Simulations- und \ac{lidar}-Projekt benutzen das Core-Projekt, um die Logik zur Steuerung des autonomen Fahrzeugs auszuführen und 
verwenden dazu Schnittstellen in Form von Implementierungen mehrerer Interfaces, 
wodurch unter anderem das Fahrzeug gesteuert und \ac{lidar} Daten ausgelesen werden können. 
Nachfolgend werden die einzelnen Projekte und die Schnittstellen, in Form der Interfaces, zwischen den Projekten näher beschrieben. 

\subsubsection{Core-Projekt}

Wie bereits beschrieben enthält das Core-Projekt die Implementierungen der verwendeten Algorithmen und die Logik zum Steuern des Fahrzeugs. 
Damit die Algorithmen auf Daten von Sensoren, sowie die Steuerung des Autos zuzugreifen kann, werden Interfaces als Schnittstellen verwendet, 
welche die Funktionen für die Algorithmen bereitstellen. Diese Interfaces werden im Core-Projekt lediglich definiert und nicht implementiert. 
Die Implementierung der Interfaces erfolgt in den Simulations- und \ac{lidar}-Projekt. 
Dort können die Interfaces so implementiert werden, dass durch Verwendung des Interfaces die richtige Aktion im jeweiligen Projekt ausgeführt wird. 
So stellt z. B. das Interface zum Auslesen der \ac{lidar} Daten im Simulations-Projekt Daten, welche den aktuellen Stand der Simulation widerspiegeln, 
und im \ac{lidar}-Projekt Daten, welche über das RPLiDAR-\ac{sdk} aus dem verbauten \ac{lidar} ausgelesen wurden, zurückgegeben werden.
Damit die Implementierungen der Interfaces an das Core-Projekt übergeben werden können und abhängig von der aktuellen Verwendung des Core-Projektes 
die richtige Implementierung verwendet wird, wird das Dependency Injection Design Pattern angewandt. 
Dieses Design Pattern besagt, dass Abhängigkeiten, wie z. B. die Logik für das Auslesen der \ac{lidar} Daten, 
ausgelagert und über festgelegt Schnittstellen von einem Injector zur Verfügung gestellt werden. 
In diesem Projekt werden die Schnittstellen in Form Interfaces festgelegt. 
Die Implementierungen der Interfaces werden bei der Initialisierung der Klasse von dem aufrufendem Code übergeben. 
Dieser fungiert hierbei als Injector. 
Die Implementierungen der Interfaces, welche die Abhängigkeiten darstellen, können nun von der Klasse verwendet werden \cite{dependencyInjection}.

Außer den Interfaces für das Auslesen der \ac{lidar} Daten und der Steuerung des Fahrzeugs befinden sich außerdem noch der Ausweichalgorithmus 
und der SLAM-Algorithmus, welcher ebenfalls durch Interfaces abstrahiert sind, damit auch diese einfach ausgetauscht werden können. 
Die gesamte Logik für das autonome Fahrzeug ist in der Klasse \textit{SelfdrivingVehicle} gebündelt. 
Diese besitzt eine Methode \textit{update}, welche in der Hauptschleife des jeweiligen Programms aufgerufen wird. 
Der Ablauf in der \textit{update} Methode, ist:

\begin{enumerate}[leftmargin=*]

    \item \textbf{Auslesen der \ac{lidar} Daten}

    Hier wird über das \ac{lidar} Interface die aktuellen Daten des \ac{lidar}s ausgelesen und für die nächsten Schritte gespeichert.

    \item \textbf{Ausführen des SLAM-Algorithmus} 
    
    Hierfür werden die ausgelesenen \ac{lidar} Daten, sowie die Odometrie Daten übergeben. 
    Je nach Implementierung der Interfaces sind die Odometrie Daten Leer, da sie nicht vorhanden sind, 
    oder werden nicht für die Ausführung des SLAM-Algorithmus verwendet. 

    \item \textbf{Ausführen des Ausweichalgorithmus}

    Dem Algorithmus wird die Karte, die Position und die Rotation des Fahrzeugs übergeben, 
    welche im Schritt davor durch den SLAM-Algorithmus berechnet wurde. 
    Daraus wird mit einem zuvor definierten Ziel ein Pfad berechnet, welcher um die erkannten Hindernisse fährt. 
    Mit dieser wird dann berechnet, wie der Motor und die Lenkung gesetzt werden, damit das Fahrzeug auf diesem Pfad fährt. 

    \item \textbf{Updaten der Motor- und Lenksteuerung} 

    In diesem Schritt werden mit den Werten aus dem vorherigen Schritt die Steuerung für den Motor und die Lenkung geändert. 
    Dies wird über das Interface zur Motor- und Lenksteuerung gemacht. 

\end{enumerate}

Da die gesamte Ausführung alle Schritte nicht jeden Durchlauf der Schleife nicht Nötig und mit der vorhandenen Hardware nicht effizient wäre, 
ist zusätzlich noch eine Überprüfung der Zeit, seit der letzten Ausführung, vorhanden. 
Die Zeitdifferenz, nach welcher die nächste Ausführung startet, kann variabel gesetzt werden, 
sollte aber zwischen 500 und 1000 ms betragen 
[\href{https://github.com/Jundy0/Studienarbeit/blob/main/src/core/src/selfdrivingVehicle.cpp}{src/core/src/selfdrivingVehicle.cpp}]. 

\subsubsection{Simulation-Projekt}

Die Simulation dient dazu, die Logik aus dem Core-Projekt zu testen, ohne dass das eigentliche Fahrzeug benötigt wird. 
Damit das Fahrzeug sowohl gesteuert als auch gesehen werden kann, was durch die Algorithmen berechnet wurde. 
Deshalb wurde die Simulation in zwei Fenster aufgeteilt, welche bei Ausführung des Programms zusammen geöffnet werden. 
Das erst Fenster, nachfolgend Control Window genannt, dient zur Steuerung des Fahrzeugs und der Simulation im allgemein. 
Es stellt in Bezug auf das autonome Fahrzeug die Realität dar. Das zweite Fenster zeigt die Ergebnisse der Algorithmen. 
Dabei wird die Karte, die Position und Rotation des Fahrzeugs auf der Karte, sowie der berechnete Pfad mit dem aktuellen Ziel angezeigt. 
In Bezug auf das autonome Fahrzeug stellt dieses Fenster die Sicht des Fahrzeugs dar. 
Die Hauptschleife der Simulation, welche das Control und Visualize Window aufruft, ist in einem Simulation-Manager enthalten. 
Dieser dient zusätzlich auch als Einstiegs- und Endpunkt der gesamten Simulation 
[\href{https://github.com/Jundy0/Studienarbeit/blob/main/src/simulation/src/simulationManager.cpp}{src/simulation/src/simulationManager.cpp}]. 
Nachfolgend wird die Funktionen und die Logik der beiden Fenster genauer erläutert. 
Außerdem wird noch SFML vorgestellt, was für die Umsetzung der grafischen Benutzeroberflächliche benutzt wurde. 
Der gesamte Ablauf der Simulation, mit Verwendung des Core-Projektes kann im Anhang \ref{fig:simulation_sequence_diagram} eingesehen werden. 

\paragraph{SFML} \mbox{}

SFML (Simple and Fast Multimedia Library) ist eine Library, mit welcher grafische Anwendungen erstellt werden können. 
Die Anwendung können dabei auf den gängigen Plattformen, wie Windows, Linux und MacOS laufen 
und können in unterschiedlichen Programmiersprachen erstellt werden, darunter auch C++ \cite{sfml}. 

\paragraph{Control Window} \mbox{}

In dem Control Window kann das simulierte Fahrzeug direkt über die \textit{WASD}-Tasten gesteuert werden. 
Außerdem können weitere Hindernisse platziert und das Ziel geändert werden, zu welchem das Fahrzeug fahren soll. 
Die Hindernisse können durch gedrückthalten der linken Maustaste und ziehen der Maus in der gewünschten Größe an dem gewünschten Ort platziert werden. 
Das Ziel kann ebenfalls durch das Benutzen der linken Maustaste an eine neue Postion platziert werden. 
Die beiden Funktionen können nicht gleichzeitig gemacht werden, weshalb durch Drücken der Leertaste zwischen den beiden Funktionen gewechselt werden muss. 

Das Control Window enthält zwei Methoden \textit{update} und \textit{render}, welche in der Hauptschleife des Simulation-Managers aufgerufen werden. 
In der \textit{update} Methode werden zunächst Events des Benutzers, wie die Steuerung des Fahrzeugs oder Schließen des Fensters, angerufen und verarbeitet. 
Danach wird die \textit{update} Methode des \textit{SelfdrivingVehicle} aus dem Core-Projekt aufgerufen, 
wodurch die Logik des autonomen Fahrzeugs ausgeführt wird. 
Da aktuell die Simulation ausgeführt wird, wird hierbei der simulierte \ac{lidar} verwendet und das simulierte Fahrzeug gesteuert. 
In der \textit{render} Methode werden die visuellen Komponenten, wie das Fahrzeug, die Hindernisse oder die Rays des simulierten Lidars, auf dem Fenster, 
über SFML, dargestellt [\href{https://github.com/Jundy0/Studienarbeit/blob/main/src/simulation/src/controlWindow.cpp}{src/simulation/src/controlWindow.cpp}]. 

\paragraph{Visualize Window} \mbox{}

Über das Visualizer Window werden die Ergebnisse ausgeführten Logik des autonomen Fahrzeugs angezeigt. 
Dazu gehören die Karte, die Position und Rotion des Fahrzeugs, welche durch den SLAM-Algorithmus berechnet wurden, 
und das Ziel mit dem berechneten Pfad, welcher durch den Ausweichalgorithmus anhand der aktuellen Karte, Position und Rotion berechnet wurden. 

Wie auch das Control Window, besitzt das Visualize Window eine \textit{update} und \textit{render} Methode,
 welche auch in der Hauptschleife des Simulation-Managers aufgerufen werden. 
 In der \textit{update} Methode werden lediglich die Events des Benutzers abgerufen und verarbeitet und in der \textit{render} Methode werden 
 wieder die visuellen Komponenten dargestellt 
 [\href{https://github.com/Jundy0/Studienarbeit/blob/main/src/simulation/src/visualizeWindow.cpp}{src/simulation/src/visualizeWindow.cpp}]. 

\subsubsection{LiDAR-Projekt}

Das \ac{lidar}-Projekt enthält das Programm, welches auf dem Raspberry Pi des autonomen Fahrzeugs ausgeführt wird und mit diesem das Fahrzeug steuert. 
Damit dies möglich ist, werden Implementierungen für das \ac{lidar} Interface und das Interface zur Steuerung des Fahrzeugs benötigt. 
Da zum Endzeitpunkt dieser Arbeit die genaue Schnittstelle mit dem Motor und der Lenkung des Fahrzeugs nicht feststeht, 
ist nur die Implementierung des \ac{lidar} Interfaces vorhanden, welche in Kapitel \ref{lidar_implementation} näher beschrieben wird. 
Voraussichtlich wäre die Schnittstelle mit dem Motor und der Lenkung durch das Setzen von Spannungen über die \ac{gpio} Pins des Raspberry Pis gelaufen. 
Hierfür könnte die Library pigpio verwendet werden.  
Mit den Implementierungen der Interfaces könnten daraufhin das \textit{SelfdrivingVehicle} verwendet werden, um die Logik des Fahrzeugs auszuführen. 
Allerdings wurde für das Programm noch keine Lösung gefunden, wie das Ziel des Fahrzeugs an den Raspberry Pi übergeben werden kann. 
Deshalb würde aktuell das Ziel fest im Code stehen oder als Commandline Argument beim Ausführen des Programms übergeben werden. 

\subsection{Erläuterung der Implementierung}

Nachdem nun der Aufbau der Implementierung erläutert wurde, werden nun einige Besonderheiten aus der Implementierung näher erläutert. 

\subsubsection{Simulation des LiDARs}

Ein Teil der Simulation des autonomen Fahrzeugs ist die Simulation des \ac{lidar}s. Hierfür wird die Methode des Raycasting verwendet, 
bei welchem Strahlen, also Vektoren, von dem Zentrum des \ac{lidar}s in alle Richtungen ausgestrahlt werden und 
die Schnittpunkte mit den Hindernissen gespeichert werden.
Umgesetzt wurde dies, indem der gesamte Umfang des \ac{lidar}s in 360 bzw. 720 Winkel unterteilt wurde, 
welche die Rays darstellen, und dann für jeden Ray die Schnittpunkte für alle Hindernisse berechnet wurden. 
Zusätzliche wurde auch der Rahmen des Fensters als hinzugefügt, da dieser die Wände des Raums und somit auch ein Hindernis darstellt. 
Da alle Hindernisse, eingeschlossen Fensterrahmen, Rechtecke sind, müssen also lediglich die Schnittpunkte zwischen einem Rechteck und einem Ray, 
also einer Geraden, berechnet werden. Dies kann weiter unterteilt werden in den Schnittpunkt zweier Geraden, da ein Rechteck aus vier Geraden besteht. 
Für diesen Zweck wurde eine Funktion erstellt, welche für diesen Fall einen Schnittpunkt berechnet, falls dieser existiert. 
In dieser wird zunächst überprüft, ob die beiden Geraden überhaupt einen Schnittpunkt besitzen oder ob diese parallel sind. 
Dies kann einfach über das Berechnen des Kreuzproduktes gemacht werden. 
Ist das Ergebnis des Kreuzproduktes gleich \(0\), sind die beiden Geraden parallel und es gibt keinen Schnittpunkt. 
Als Grundlage für eigentliche Berechnung des Schnittpunktes wurde die Formel zur Berechnung des Schnittpunktes zweier Geraden benutzt, 
in welcher lediglich zwei Geraden in Parameterform \((Stuetzvektor + Parameter * Richtungsvektor)\) gleichgestellt werden. 
Da für die Berechnung der Schnittpunkte angenommen wird, dass der \ac{lidar} bei \((0, 0)\) liegt, kann dieser in der Gleichung weggelassen werden. 

\[
s * 
\begin{pmatrix}
    rayDirection_x \\ 
    rayDirection_y
\end{pmatrix}
= 
\begin{pmatrix}
    v1_x \\ 
    v1_y
\end{pmatrix}
+ t * 
\begin{pmatrix}
    v12_x \\ 
    v12_y
\end{pmatrix}
\]

Aus dieser Formel wird ein lineares Gleichungssystem gemacht, welches anschließend nach dem Parameter \(t\) aufgelöst wird.

\[
t = \frac{v1_x * rayDirection_x - v1_y * rayDirection_y}{v12_x * rayDirection_x - v12_y * rayDirection_y} 
\]

Durch Einsetzen in eine der beiden Gleichungen des LGS kann auch der Parameter \(s\) berechnet werden. 

\[
s = \frac{v1_x + t * v12_x}{rayDirection_x};
s = \frac{v1_y + t * v12_y}{rayDirection_y}
\]

Nun muss überprüft werden, ob der Punkt nicht nur auf beiden Geraden, sondern auch zwischen den beiden Punkten des Rechtecks, 
sowie vor dem \ac{lidar} liegt. 
Dafür wird geschaut, ob der Wert von \(t\) (Parameter für die Kante des Rechtecks) zwischen \(0\) und \(1\) liegt. 
Außerdem wird geschaut, ob der Wert von \(s\) (Parameter für den Ray) größer oder gleich \(0\) ist. 
Sollte beides gegeben sein, wird der Schnittpunkt zur Liste aller Schnittpunkte des aktuellen Rays hinzugefügt 
[\href{https://github.com/Jundy0/Studienarbeit/blob/main/src/simulation/src/intersection.cpp}{src/simulation/src/intersection.cpp}]. 

\begin{lstlisting}[caption={Berechnung des Schnittpunktes zweier Geraden},label={lst:schnittpunkt_zweier_geraden},language={C++}]
bool intersects(const sf::Vector2f &rayOrigin, const sf::Vector2f &rayDirection, const sf::Vector2f &p1, const sf::Vector2f &p2, std::vector<sf::Vector2f> &intersectionPoints)
{
    sf::Vector2f v1 = p1 - rayOrigin;
    sf::Vector2f v2 = p2 - rayOrigin;
    sf::Vector2f v12 = v2 - v1;

    const float cross = crossProduct(rayDirection, v12);

    if (cross == 0)
    {
        return false; // ray and edge are parallel
    }

    const float t = crossProduct(v1, rayDirection) / cross; // Parameter for Edge
    const float s = rayDirection.x != 0 ? ((v1.x + t * v12.x) / rayDirection.x) : ((v1.y + t * v12.y) / rayDirection.y); // Parameter for Ray

    if (t >= 0 && t <= 1 && s >= 0) // Is between points and in positive direction of Ray
    {
        const sf::Vector2f intersectionPoint = v1 + rayOrigin + t * v12;
        intersectionPoints.push_back(intersectionPoint);

        return true;
    }

    return false;
}
\end{lstlisting}

Nachdem alle Schnittpunkte berechnet wurden, muss anschließend der Schnittpunkt bestimmt werden, der am nächsten am \ac{lidar}, 
also dem Stützvektor des Rays, ist. 
Dafür wird die Liste mit den Schnittpunkten nach dem Abstand zum \ac{lidar} sortiert, 
indem der quadrierte Abstand zwischen Schnittpunkt und \ac{lidar} berechnet. 
Der Vorteil des quadrierten Abstandes gegenüber dem normalen Abstand ist, dass die Reihenfolge einzelner Punkte gleich bleibt, 
aber die Berechnung über die Wurzel vermieden wird, welche vergleichsweise aufwendig ist \cite{stackexchangeAreThereAny2012}. 
Nach der Sortierung kann dann das erste Element der Liste als Schnittpunkt für diesen Ray verwendet werden. 
Da aktuell nur die Koordinaten des Schnittpunktes bekannt sind, ein \ac{lidar} aber nur den Winkel und die Entfernung eines Punktes kennt, 
müssen für die Simulation diese Werte noch berechnet werden 
[\href{https://github.com/Jundy0/Studienarbeit/blob/main/src/simulation/src/intersection.cpp}{src/simulation/src/intersection.cpp}, 
\href{https://github.com/Jundy0/Studienarbeit/blob/main/src/simulation/src/lidarSensorSim.cpp}{src/simulation/src/lidarSensorSim.cpp}]. 

\subsubsection{Implementierung des LiDARs mit der RPLIDAR SDK} \label{lidar_implementation}

Wie bereits in der Technologie-Entscheidung beschrieben, wird für die Ansteuerung des \ac{lidar}s das RPLIDAR-\ac{sdk} verwendet. 
Zusätzlich wird die Library pigpio benutzt, um die \ac{gpio} Pins des Raspberry Pis zu verwenden. Dies wird benötigt, 
da der Motor des \ac{lidar} über ein PWM-Signal gesteuert wird. 
Nach dem Auslesen müssen die erhaltenen Werte vor der Weiterverwendung noch zu den richtigen Werten konvertiert werden. 
Zur einfacheren Übertragung werden die Werte für den Winkel und den Abstand bei das \ac{sdk} als unsigned 16 bit bzw. 32 bit Integer gespeichert. 
Um die Werte wieder in eine Kommazahl zu konvertieren, 
wird der Abstand durch \(4000\) und der Winkel mal \(90\) und durch \(16384\) (eine 1 um 14 Stellen nach links geshifted) geteilt 
\cite{RplidarSDK2023}. 
Mit den konvertierten Werten kann nun auch der \(x\) und \(y\) Wert des Punktes über \(cos\) und \(sin\) gerechnet werden 
[\href{https://github.com/Jundy0/Studienarbeit/blob/main/src/lidar/src/a1lidarSensor.cpp}{src/lidar/src/a1lidarSensor.cpp}]. 

\begin{lstlisting}[caption={Auslesen der LiDAR Daten},label={lst:auslesen_lidar},language={C++}]
void A1LidarSensor::getScanData(lidar_point_t *data, size_t count)
{
    rplidar_response_measurement_node_hq_t scanData[count];
    u_result res = drv->grabScanDataHq(scanData, count);

    if (res == RESULT_OK)
    {
        printf("Grabbed scan data\n");
    }
    else
    {
        printf("Failed to grab scan data\n");
        printf("Error code: %d\n", res);
        return;
    }

    for (int i = 0; i < count; i++)
    {
        const double angle = scanData[i].angle_z_q14 * (90.f / 16384.f);
        const double distance = scanData[i].dist_mm_q2 / 4000.0f;
        data[i].radius = distance;
        data[i].angle = angle;
        data[i].x = distance * cos(angle);
        data[i].y = distance * sin(angle);
        data[i].quality = scanData[i].quality;
        data[i].valid = scanData[i].quality > 7;
    }
}
\end{lstlisting}

\subsubsection{Umsetzung von SLAM}
\label{slamImplementierung}

Zur Umsetzung einer Lösung des \ac{slam} Problems, welches in Kapitel \ref{slam} beschrieben wurde, sind zwei Implementierungen notwendig.
Zum einen die Implementierung einer Möglichkeit eine Karte der unbekannten Umgebung aufzubauen 
und zum anderen die Implementierung einer Möglichkeit die Position des Fahrzeugs innerhalb dieser Karte zu bestimmen.

Die gesamte Implementierung wird durch den \textit{SlamHandler} vom Rest des Codes abstrahiert.
Somit wird für jedes Update lediglich die update-Funktion des \textit{SlamHandler} durch das \textit{SelfdrivingVehicle} aufgerufen.
Das entsprechende Sequenzdiagramm befindet sich im Anhang. \ref{fig:slam_sequence_diagram}

\paragraph{\textit{Particle}-Klasse} \mbox{} \\
Ursprünglich war geplant \ac{slam} mittels eines Partikel Filter umzusetzen.
Ein Beispiel für einen solchen Filter ist der FastSLAM-Algorithmus \cite{montemerlo2002fastslam}.
Dieser Plan wurde jedoch, aufgrund der hohen Komplexität eines solchen Filters und der begrenzten Rechenleistung und Zeit, verworfen.

Im aktuellen Stand wird somit nur ein einzelnes Partikel bei der Initialisierung des \textit{SlamHandler} erstellt.
Das \textit{Particle}-Objekt repräsentiert den aktuellen Zustand des Roboters.
In ihm werden Position, Rotation und Karte, sowie eine Instanz des \textit{PclHandler} gespeichert.
[\href{https://github.com/Jundy0/Studienarbeit/blob/main/src/core/src/particle.cpp}{src/core/src/particle.cpp}]

Neben Get-Methoden für die Karte, Position und Rotation gibt es Methoden zum Aktualisieren des Partikels.
Diese aktualisieren sowohl die Karte, als auch die Position und Rotation des Partikels.
Auf den Ablauf wird in dem Kapitel \ref{position_update} eingegangen.

\paragraph{Erstellung einer Karte} \mbox{} \\
Wie bereits in Kapitel \ref{mapping} beschrieben, wird für den Aufbau der Karte ein Occupancy Grid verwendet.
Hierzu gibt es die entsprechende Klasse \textit{OccupancyGrid}.
[\href{https://github.com/Jundy0/Studienarbeit/blob/main/src/core/src/occupancyGrid.cpp}{src/core/src/occupancyGrid.cpp}]

Eine wichtige Unterscheidung ist die zwischen Karte und Grid.
Die Karte selbst wird lediglich für die Berechnungen benutzt.
Sie beschreibt nur Grenzwerte für Koordinaten und somit einen Bereich in dem der Roboter und die gescannten Punkte sein können.

Sämtliche Informationen über den Status der Karte an sämtlichen Koordinaten werden in dem Occupancy Grid gespeichert.
Man kann sich das Grid als Etwas, dass auf der virtuellen Karte darauf liegt vorstellen.
Sowohl die Maße der Karte, als auch die Maße des Grid werden mittels Konstanten in einer Settings-Datei festgelegt.
[\href{https://github.com/Jundy0/Studienarbeit/blob/main/src/core/include/settings.h}{src/core/include/settings.h}]

Das Grid selbst, sowie eine Kopie des Grids, welcher per shared Pointer über die Get-Methode erfragt werden kann, werden als Matrix gespeichert.
Da es sich hier um sehr große Matrizen handelt, wurde für die Handhabung dieser die Bibliothek Eigen3 verwendet.
Eigen ist eine vielseitig einsetzbare und schnelle Bibliothek für die Handhabung von Vektoren und Matritzen.

Die beiden Matrizen werden im Konstruktor des \textit{OccupancyGrid} als \textit{probMap} und \textit{probMapCpy} deklariert und mit Nullen initialisiert.
\newline

Jedes mal wenn neue Daten mittels Scan erstellt werden, wird die Funktion \textit{updateProbMap} aufgerufen.
Hierbei werden sowohl die Scan-Daten, in Form einer Matrix mit einem Punkt pro Zeile, 
als auch die aktuelle Position des Roboters in der Karte und die Rotation des Roboters in Grad übergeben.

Für die Aktualisierung des Grid werden die Scan-Daten genutzt um zwei Arten von Punkten zu berechnen.
Punkte an denen ein Hindernis ist und Punkte welche frei befahrbar sind.
Hierzu wird die Methode \textit{getPoints} genutzt. \ref{lst:getPoints}

Zuerst werden zwei RowMajor Matritzen deklariert.
Die Option RowMajor sorgt dafür, dass die Matrix Zeilenweise in den Speicher geschrieben wird.
Da jede Zeile einem Punkt entspricht, verkürzt das die Laufzeit beim Auslesen der Punkte aus der Matrix enorm.

Danach wird jeder Punkt des Scans von den Polarkoordinaten, welche der \ac{lidar}-Scan zurückgibt, in kartesische Koordinaten umgerechnet.
Hierbei werden die Punkte zusätzlich in ein globales Koordinatensystem übertragen.
Dies geschieht durch die Addition der Rotation des Roboters auf den Winkel der Polarkoordinate
und der Addition der Position des Roboters auf die resultierenden kartesischen Koordinaten.
Die gescannten Punkte werden dann an die Matrix für die belegten Punkte angehangen.

Die frei befahrbaren Punkte sind alle die Punkte, welche sich zwischen dem Sensor und dem gescannten Hindernis befinden.
Zur Berechnung dieser Punkte wird der Bresenham-Algorithmus verwendet.
Der entsprechenden Methode werden die X- und Y-Koordinaten des Roboters, sowie die Koordinaten des Zielpunktes übergeben.
Zurückgegeben wird eine Matrix, welche Koordinaten sämtlicher Punkte enthält, die sich auf einer Linie zwischen den übergebenen Punkten befinden.
Diese werden der Matrix für frei befahrbare Punkte angehangen.

Am Ende werden beide Matrizen als Pair zurückgegeben.

\begin{lstlisting}[caption={Berechnung occPoints und freePoints},label={lst:getPoints},language={C++}]
std::pair<Eigen::MatrixX2d, Eigen::MatrixX2d> OccupancyGrid::getPoints(const Eigen::MatrixX2d &scan, const Eigen::RowVector2d &robPos, double robRotAngle)
{
    // Define X,2 Matrices for occupied and free points.
    // RowMajor stores the matrix row wise in memory and is used since each point is represented by one row in the matrix.
    Eigen::Matrix<double, -1, 2, Eigen::RowMajor> occPoints;
    Eigen::Matrix<double, -1, 2, Eigen::RowMajor> freePoints;

    Eigen::Matrix<double, -1, 2, Eigen::RowMajor> data = scan;

    for (int i = 0; i < data.rows(); i++)
    {
        // Converts each of the polar points from the scan data into cartesian and stores it in occPoints
        Eigen::RowVector2d polarPoint = data.block<1, 2>(i, 0);
        Eigen::RowVector2d cartPoint = polarToCartesian(polarPoint, robPos, robRotAngle);
        occPoints.conservativeResize(occPoints.rows() + 1, Eigen::NoChange);
        occPoints.row(occPoints.rows() - 1) = cartPoint;

        // Calculates free points using bresenham and stores them into freePoints
        Eigen::MatrixX2d bresenhamPoints = bresenham(robPos[0], robPos[1], cartPoint[0], cartPoint[1]);
        for (int j = 0; j < bresenhamPoints.rows(); j++)
        {
            freePoints.conservativeResize(freePoints.rows() + 1, Eigen::NoChange);
            freePoints.row(freePoints.rows() - 1) = bresenhamPoints.block<1, 2>(j, 0);
        }
    }
    // Returns both matrices
    return std::make_pair(occPoints, freePoints);
}
\end{lstlisting}

Die berechneten Punkte werden dann verwendet um die \textit{probMap} bzw. das Grid zu aktualisieren. \ref{lst:update-prob-map}

Für jeden Punkt der Matrix mit belegten Punkten, werden X- und Y-Koordinate ausgelesen.
Da die Koordinaten in der Karte liegen, das Grid jedoch kleiner ist, müssen die Koordinaten der Punkte angepasst werden.
Daher werden diese durch den Faktor, um den die Map größer ist als das Grid, geteilt.
Zusätzlich ist darauf zu achten, dass die Matrix per (Zeile,Spalte) adressiert wird.
Da die Zeilen der y-Achse entsprechen und die Spalten der x-Achse muss die Adressierung entsprechen per (Y,X) erfolgen.
Der Wert in der Matrix wird, solange er unter einem Grenzwert liegt, um ein Delta erhöht.
Wird der Grenzwert erreicht, wird der Punkt im Grid als belegt angesehen.
Auch der Grenzwert und das Delta können in der oben erwähnten Settings-Datei festgelegt werden.

Derselbe Vorgang wird für die freien Punkte wiederholt.
Einziger Unterschied ist, dass der Wert im Grid gesenkt statt erhöht wird.
Auch hier können Grenzwert und Delta per Settings-Datei festgelegt werden.

\begin{lstlisting}[caption={Auschnitt aus updateProbMap},label={lst:update-prob-map},language={C++}]
// Updates values in the probMap for occupied points
for (int i = 0; i < occPoints->rows(); i++)
{
    // Gets x and y for each point from the matrix
    // Must be divided by the ratio between map and grid since the grind is a fraction of the size to allow for some error
    // (e.g. 1x1 in the grid is 10x10 in the map so all the points on the map that lay in this 10x10 area will change the value of probability at that single point in the grid)
    int x = occPoints->coeff(i, 0) / (MAP_WIDTH / GRID_WIDTH);
    int y = occPoints->coeff(i, 1) / (MAP_WIDTH / GRID_WIDTH);

    if ((*probMap)(y, x) < PROB_OCC_THRES)
        (*probMap)(y, x) += DELTA_OCC;
}

// Updates values in the probMap for free points
for (int i = 0; i < freePoints->rows(); i++)
{
    // Gets x and y for each point from the matrix
    // Must be divided by the ratio between map and grid since the grind is a fraction of the size to allow for some error
    int x = freePoints->coeff(i, 0) / (MAP_WIDTH / GRID_WIDTH);
    int y = freePoints->coeff(i, 1) / (MAP_WIDTH / GRID_WIDTH);

    if ((*probMap)(y, x) > PROB_FREE_THRES)
        (*probMap)(y, x) += DELTA_FREE;
}
\end{lstlisting}

\paragraph{Aktualisierung der Position und Rotation} \mbox{} \\
\label{position_update}Zur Aktualisierung der Position wird die Methode \textit{updatePosition} des \textit{Particle} aufgerufen.
Dieser werden die notwendigen Werte für Positionsänderung und Rotationsänderung übergeben, welche dann auf die aktuellen Werte addiert werden.

Die Berechnung der Werte kann auf zwei Arten erfolgen. \ref{lst:slam-handler-update}
\begin{enumerate}
    \item \textbf{Auslesen aus der Simulation} \\
    Die erste Möglichkeit an die Werte zu kommen ist das Auslesen dieser aus der Simulation.
    Hierzu wird der Methode \textit{update} des \textit{Particle} nur der aktuelle Scan, 
    sowie die Werte für Positionsänderung und Rotationsänderung übergeben.
    Diese wurden vorher aus der Simulation ausgelesen und über die Update-Funktion des \textit{SlamHandler} an diesen übergeben.
    Der Scan wird für das Updaten der Karte benötigt.
    Die Werte für Positionsänderung und Rotationsänderung werden einfach an die Methode \textit{updatePosition} des \textit{Particle} weitergegeben.

    Diese Methode ist ausschließlich für Debugzwecke gedacht und kann nur in Kombination mit der Simulation verwendet werden.

    \item \textbf{Berechnung mittels \ac{icp}} \\
    Die zweite Möglichkeit ist das Berechnen der Transformationsmatrix zwischen vorherigem Scan und dem Aktuellen.
    Hierzu wird ein \ac{icp}-Algorithmus der \ac{pcl}-Bibliothek verwendet.
    Auch hier wird die Methode \textit{update} des \textit{Particle} aufgerufen.
    Neben dem aktuellen Scan wird diesmal jedoch auch der vorherige Scan mit übergeben.
    Die Werte für Positionsänderung und Rotationsänderung werden zwar ebenfalls übergeben, allerdings geschieht das nur für Debugzwecke.
    Die übergebenen Werte haben auf die weitere Berechnung und die Werte welche letztendlich an die \textit{updatePosition} Methode übergeben werden keinen Einfluss.
\end{enumerate}

Die Auswahl der genutzten Möglichkeit wird über ein Flag in der Header-File des \textit{SlamHandler} ermöglicht.
[\href{https://github.com/Jundy0/Studienarbeit/blob/main/src/core/include/slamHandler.h}{src/core/include/slamHandler.h}]

\begin{lstlisting}[caption={Auschnitt aus der Methode \textit{update} des \textit{SlamHandler}},label={lst:slam-handler-update},language={C++}]
void SlamHandler::update(lidar_point_t *data, const Eigen::RowVector2d &positionDiff, double rotationDiff)
{
    [...]

    // If update is called the first time no lastScan exists so update of the particle gets skipped
    if (this->initial)
    {
        this->initial = false;
    }
    else
    {
        if (this->useOdometry)
        {
            this->particle.update(this->currentScan, positionDiff, rotationDiff); // Only Ododmetry, no ICP
        }
        else
        {
            this->particle.update(this->lastScan, this->currentScan, positionDiff, rotationDiff); // Uses ICP. Odometry data gets used for debug output
        }
    }

    [...]
} 
\end{lstlisting}

\paragraph{Umsetzung mittels \ac{icp}} \mbox{}\\
Die Methode zur Berechnung der Werte für Positionsänderung und Rotationsänderung,
welche bei Verwendung des echten \ac{lidar} zum Einsatz kommen soll, verwendet einen \ac{icp}-Algorithmus.

Als Interface zur Kommunikation zwischen \textit{Particle} und der \ac{pcl}-Bibliothek, gibt es die Klasse \textit{pclHandler}.
[\href{https://github.com/Jundy0/Studienarbeit/blob/main/src/core/src/pclHandler.cpp}{src/core/src/pclHandler.cpp}]

Durch Aufruf der Methode \textit{computeTransformation} werden die Werte für Positionsänderung und Rotationsänderung, 
basierend auf den letzten beiden Scans, berechnet und als \textit{TransformationComponents} \ref{lst:trans-comp} zurückgegeben.

\begin{lstlisting}[caption={Aufbau \textit{TransformationComponents}},label={lst:trans-comp},language={C++}]
typedef struct
{
    /// @brief A vector composed of global x and y values
    Eigen::RowVector2d translation_vector;
    /// @brief A rotation angle in deg
    double rotation_angle;
} TransformationComponents;
\end{lstlisting}

Die, für den \ac{icp}-Algorithmus verwendeten Parameter, sowie typedefs, welche den Code übersichtlicher machen, befinden sich in der Header-Datei.
[\href{https://github.com/Jundy0/Studienarbeit/blob/main/src/core/include/pclHandler.h}{src/core/include/pclHandler.h}]

Beim Aufruf der Methode \textit{computeTransformation} werden die Scans, in Form zweier Matrizen mit Punkten in Polarkoordinaten, 
sowie die Rotation des Roboters zum Zeitpunkt des ersten Scans übergeben.
Da \ac{pcl} Punktewolken für sämtliche Berechnungen verwendet, müssen diese Matrizen zuerst in Punktewolken umgewandelt werden.

Hierzu werden die Punkte zuerst in ein kartesisches Koordinatensystem überführt.
Dabei gilt zu beachten, dass die Rotation des Roboters auf den Winkel der Punkte addiert wird.
Damit wird sichergestellt, dass die resultierenden Wolken nach den globalen Koordinatenachsen der Karte ausgerichtet sind.
Genaueres hierzu in Kapitel \ref{coordinates_problem}.

Nachdem die Scan-Daten korrekt ausgerichtet und in ein Punktewolken-Format gebracht wurden, wird der \ac{icp}-Algorithmus durchgeführt. \ref{lst:icp}

Zuerst wird ein Pointer auf eine PointCloud erzeugt, in welchen die vom Algorithmus transformierte Punktewolke geschrieben wird.
Dieser wird mit dem Pointer der PointCloud des ersten Scans gleichgesetzt.
Das sorgt dafür, dass das Ergebnis des Algorithmus die Punktewolke des ersten Scans überschreibt.
Somit wird bei der Durchführung einer weiteren Iteration des Algorithmus, das Ergebnis der vorherigen Iteration als Input genommen.

Außerdem wird die Transformationsmatrix und ein Counter initialisiert.

Danach beginnt eine do-while Schleife.
So wird sichergestellt, dass der Algorithmus mindestens einmal durchlaufen wird.
Sollte das Ergebnis zu ungenau sein, wird der Algorithmus erneut durchgeführt.
Das geschieht so lange, bis das Ergebnis eine gewünschte Genauigkeit erreicht hat 
oder die maximale Anzahl an Iterationen erreicht wurde.

Der Ablauf einer Iteration ist recht simpel.
Die, in der Header-Datei festgelegten Parameter werden gesetzt.
Hierbei wird die \textit{MAX\_CORRESPONDENCE\_DISTNACE\_ICP} mit jeder Iteration verringert.
Dies geschieht weil davon ausgegangen wird, dass nach jeder Iteration der Abstand zwischen den beiden Punktewolken verringert wurde.

Danach werden Input und Target des Algorithmus gesetzt und der Algorithmus wird ausgeführt.
Genauere Informationen über den Ablauf des Algorithmus und die Möglichkeiten finden sich in der PCL-Dokumentation \cite{pcl2024icp}.
Das Ergebnis ist eine transformierte Punktewolke, welche an den dafür erstellten Pointer geschrieben wird.
Im Optimalfall stimmt diese mit der Target PointCloud überein.

Als Nächstes wird die Transformationsmatrix ausgelesen.
Diese beschreibt die Transformation die nötig ist um die Source Punktewolke in die, 
vom Algorithmus berechnete Punktewolke, zu transformieren.
Sie wird mit der Transformationsmatrix der vorherigen Iteration multipliziert.
Das Resultat ist eine Kombination beider Matrizen.
Diese kann auf die ursprüngliche Punktewolke des ersten Scans angewendet werden, 
um das Ergebnis der aktuellen Iteration zu erhalten.

Zusätzlich wird noch der Counter für die Iterationen erhöht.
\newline

Nachdem eines der Abbruchkriterien der Schleife erfüllt ist, werden zusätzliche Informationen der finalen Iteration ausgelesen und ausgegeben.
Einmal die Information, ob der Algorithmus überhaupt erfolgreich abgeschlossen hat.
Dann der Fitness-Score des Ergebnisses. 
Dieser ist nichts Anderes als die mittlere, quadratische Abweichung der Konvergenzen.
Außerdem wird noch die Information ausgegeben, aus welchem Grund der Algorithmus beendet hat.
Mögliche Gründe sind z.B. das Erreichen der maximalen Anzahl an internen Iterationen
oder das Unterschreiten eines Schwellwerts, welche über die Parameter festgelegt wurden.

Abschließend wird die finale Transformationsmatrix zurückgegeben.

\begin{lstlisting}[caption={Implementierung des \ac{icp}-Algorithmus},label={lst:icp},language={C++}]
Eigen::Matrix4f PclHandler::computeAlignment(const PointCloud::Ptr &sourcePoints, const PointCloud::Ptr &targetPoints)
{
    // Create ICP instance
    pcl::IterativeClosestPoint<PointT, PointT> icp;

    // Set registration output to the pointer of the source cloud
    // This overwrites the source cloud with the result of the icp so it can be used for another iteration
    PointCloud::Ptr registrationOutput = sourcePoints;

    // Create initial transformation matrix that does nothing
    Eigen::Matrix4f tfMatrix;
    tfMatrix << 1, 0, 0, 0,
        0, 1, 0, 0,
        0, 0, 1, 0,
        0, 0, 0, 1;

    int iterations = 0;

    // Loop ICP until fitness score is under a threshhold or other criteria is met
    do
    {
        // Set ICP parameter
        icp.setMaximumIterations(MAX_ITERATIONS_ICP);
        icp.setMaxCorrespondenceDistance(MAX_CORRESPONDENCE_DISTANCE_ICP - (iterations * 20));
        icp.setRANSACOutlierRejectionThreshold(OUTLIER_REJECTION_THRESHOLD_ICP);
        icp.setTransformationEpsilon(TRANSFORMATION_EPSILON_ICP);
        icp.setTransformationRotationEpsilon(ROTATION_EPSILON_ICP);
        icp.setEuclideanFitnessEpsilon(EUCLIDEAN_FITNESS_EPSILON_ICP);

        // Set ICP source and target point cloud
        icp.setInputSource(registrationOutput);
        icp.setInputTarget(targetPoints);

        // Run ICP
        icp.align(*registrationOutput);

        // Get transformation matrix and store it to tfMatrix
        // If icp is not run for the first time the result gets added onto the results from previous iterations
        tfMatrix = icp.getFinalTransformation() * tfMatrix;

        iterations++; 

    } while (icp.getFitnessScore() > ICP_FITNESS_THRESHOLD && iterations < ICP_MAX_NR_CORRECTIONS);

    // Output of ICP results
    std::cout << "Converged: " << (bool)icp.hasConverged() << std::endl;
    std::cout << "Fitness: " << icp.getFitnessScore() << std::endl;
    std::cout << "Reason: " << convCrit[icp.getConvergeCriteria()->getConvergenceState()] << std::endl;

    return tfMatrix;
} 
\end{lstlisting}

Mittels der Methode \textit{extractTransformationComponents} können die relevanten Werte aus der berechnete Transformationsmatrix ausgelesen
und in \textit{TransformationComponents} umgewandelt werden.

Diese werden an das \textit{Particle} zurückgegeben und dort verwendet, um die Position zu aktualisieren.

\subsubsection{Umsetzung des Ausweichalgorithmus}

Ein wichtiger Teil des autonomen Fahrzeugs ist Umfahren oder Ausweichen von Hindernissen. 
In dieser Arbeit wird hierfür ein Pathfinding-Algorithmus verwendet, welcher mit der Karte, Position und Rotation des Fahrzeugs, 

welche durch SLAM mit den \ac{lidar} Daten generiert wurde, sowie einem Zielpunkt, einem Pfad um die erkannten Hindernisse findet. 
Aktuell wird hierfür der A*-Pathfinding-Algorithmus verwendet. Dieser arbeitet mit einer Liste an Punkten, welche die abgelaufenen Punkte beinhaltet. 
Außerdem wird mit drei Arten von Kosten gearbeitet, welche einem Punkt zugewiesen werden und den Aufwand für den Pfad widerspiegeln, 
wenn dieser Punkt im Pfad enthalten ist:

\begin{enumerate}[leftmargin=*]

\item \textbf{g-Cost: die bisherigen Kosten}

Die Kosten, die benötigt werden, um zu diesem Punkt zu gelangen. 

\item \textbf{h-Cost: die geschätzten Kosten}

Die Kosten, die voraussichtlich noch zum Ziel auftreten. Hierfür wird eine Heuristik-Funktion zur Berechnung der Kosten verwendet. 
Diese rechnet den Abstand zwischen dem aktuellen Punkt und dem Ziel aus. 
Hierfür wird in der Regel die Euklidische Distanz (Luftlinie) oder die Manhattan Distanz (Summe der absoluten Differenzen) verwendet. 

\item \textbf{f-Cost: die gesamten Kosten}

Die Kosten, die entstehen, wenn g-Cost und h-Cost summiert werden. 

\end{enumerate}

Bei einer Schleife wird der Punkt mit dem niedrigsten f-Cost aus der Liste genommen und überprüft, ob dieser dem Ziel entspricht. 
Sollte dies der Fall sein, kann der Algorithmus abbrechen und den Pfad rekonstruieren. 
Entspricht der Punkt nicht dem Ziel, werden die Nachbarn des Punktes zur Liste hinzugefügt, falls an diese keine Hindernisse sind. 
Für die Nachbarn werde die Kosten berechnet, wobei für g-Cost die g-Cost des aktuellen Punktes genommen und um eins erhöht wird. 
Außerdem wird der aktuelle Punkt als Parent-Punkt alle Nachbarn gesetzt. 
Die Schleife wird nun so lange wiederholt, bis keine Einträge mehr in der Liste sind oder der Punkt für das Ziel gefunden ist. 
Um den Pfad zu rekonstruieren wir von dem Ziel die jeweiligen Parent-Punkte nachgefahren, bis der Startpunkt erreicht wurde \cite{hartFormalBasisHeuristic1968}. 

Umgesetzt wurde der Algorithmus mit einer Priority Queue, welche immer das Element mit der geringsten f-Cost zurückgibt.
Außerdem wird jeweils eine Matrix für f-Cost, g-Cost und das Festhalten der Parent-Punkten benutzt. 
In der Schleife werden Nachbarn nur zur Priority Queue hinzugefügt, wenn dies innerhalb der Karte und der Wert an diesem Punkt kleiner als \(PROB\_OCC\) ist, 
was bedeutet, dass dort wahrscheinlich kein Hindernis vorhanden ist. 
Da ein Punkt auch über mehrere Punkte angefahren werden kann, wird der Punkt nur in die Priority Queue hinzugefügt, 
wenn der g-Cost größer ist, als der aktuell in der Matrix hinterlegte Wert 
[\href{https://github.com/Jundy0/Studienarbeit/blob/main/src/core/src/evasionAStar.cpp}{src/core/src/evasionAStar.cpp}]. 

\begin{lstlisting}[caption={Pathfinding mit A*},label={lst:astar-pathfinding},language={C++}]
void EvasionAStar::execute()
{
    size_t rows = this->map->rows();
    size_t cols = this->map->cols();
    auto cmp = [](std::pair<double, Eigen::RowVector2d> left, std::pair<double, Eigen::RowVector2d> right)
    { return left.first > right.first; };
    std::priority_queue<std::pair<double, Eigen::RowVector2d>, std::vector<std::pair<double, Eigen::RowVector2d>>, decltype(cmp)> openSet(cmp);

    Eigen::MatrixXd gScore = Eigen::MatrixXd::Constant(rows, cols, INF);
    Eigen::MatrixXd fScore = Eigen::MatrixXd::Constant(rows, cols, INF);
    Eigen::Matrix<Eigen::RowVector2d, Eigen::Dynamic, Eigen::Dynamic> parent(rows, cols);

    openSet.push({0, this->origin});
    gScore(ROUND(this->origin.x()), ROUND(this->origin.y())) = 0;
    fScore(ROUND(this->origin.x()), ROUND(this->origin.y())) = this->heuristic(this->origin, this->destination);
    parent(ROUND(this->origin.x()), ROUND(this->origin.y())) = {-1, -1};

    while (!openSet.empty())
    {
        Eigen::RowVector2d current = openSet.top().second;
        openSet.pop();

        if (current == this->destination)
        {
            while (current != Eigen::RowVector2d(-1, -1))
            {
                this->path.push_back(current);
                current = parent(ROUND(current.x()), ROUND(current.y()));
            }

            std::reverse(this->path.begin(), this->path.end());

            return;
        }

        std::vector<Eigen::RowVector2d> neighbors = {
            current + Eigen::RowVector2d(-1, 0),
            current + Eigen::RowVector2d(1, 0),
            current + Eigen::RowVector2d(0, -1),
            current + Eigen::RowVector2d(0, 1),
        };

        for (const auto &neighbor : neighbors)
        {
            size_t x = ROUND(neighbor.x());
            size_t y = ROUND(neighbor.y());

            if (x >= 0 && x < rows && y >= 0 && y < cols && this->isFree(x, y))
            {
                double tentativeGCost = gScore(ROUND(current.x()), ROUND(current.y())) + 1;

                if (tentativeGCost < gScore(x, y))
                {
                    parent(x, y) = current;
                    gScore(x, y) = tentativeGCost;
                    fScore(x, y) = gScore(x, y) + this->heuristic(neighbor, this->destination);
                    openSet.push({fScore(x, y), neighbor});
                }
            }
        }
    }
}
\end{lstlisting}

\paragraph{Obstacle Inflation} \mbox{}

Damit das autonome Fahrzeug auch wirklich den Weg fahren kann und z. B. nicht bei einer Lücke zwischen zwei Hindernissen stecken bleibt, 
wird eine Methode benutzt, bei welcher die Hindernisse künstlich vergrößert werden. Diese Methode wird auch Obstacle Inflation genannt. 
Angewendet wird diese Methode, indem vor dem eigentlich Pathfinding-Algorithmus durch dir Karte iteriert wird und bei allen Punkten in der Karte, 
die einen Wert größer als \(PROB\_OCC\) haben, also an welchen sich wahrscheinlich ein Hindernis befindet, 
die umliegenden Punkte den künstlichen Wert \(INFLATED\) bekommen. Dabei gilt \(PROB\_OCC > INFLATED > 0\). 
Dieser Wert darf aber nur für Punkte gegeben werden, die einen Wert kleiner als \(PROB\_OCC\) haben, also an welchen sich wahrscheinlich kein Hindernis befindet. 
Ansonsten würde die Information über das Vorkommen er richtigen Hindernisse verloren gehen \cite{fernandesOrientationEnhancedAstar2015}. 
Damit nun der Pfad nicht durch die künstlichen Hindernisse geht, muss bei der Ausführung des Pathfinding-Algorithmus in der Überprüfung, 
ob der Punkt belegt ist, geschaut werden, ob der Wert an dem Punkt größer \(INFLATED\) und nicht mehr größer \(PROB\_OCC\) ist 
[\href{https://github.com/Jundy0/Studienarbeit/blob/main/src/core/src/evasionControl.cpp}{src/core/src/evasionControl.cpp}]. 

\begin{lstlisting}[caption={Obstacles Inflation},label={lst:inflateObstacles},language={C++}]
void EvasionControl::inflateObstacles()
{
    const size_t rows = this->map->rows();
    const size_t cols = this->map->cols();

    for (size_t i = 0; i < rows; ++i)
    {
        for (size_t j = 0; j < cols; ++j)
        {
            if ((*this->map)(i, j) >= PROB_OCC)
            {
                for (int x = -VEHICLE_RADIUS; x <= VEHICLE_RADIUS; ++x)
                {
                    const int circleValue = std::round(VEHICLE_RADIUS * std::cos(std::abs(x) * M_PI / (2 * VEHICLE_RADIUS)));
                    for (int y = -circleValue; y <= circleValue; ++y)
                    {
                        int nx = i + x;
                        int ny = j + y;
                        if (nx >= 0 && ny >= 0 && nx < rows && ny < cols && (*this->map)(nx, ny) < PROB_OCC)
                        {
                            (*this->map)(nx, ny) = INFLATED;
                        }
                    }
                }
            }
        }
    }
}
\end{lstlisting}

\paragraph{Natürliches Fahrtverhalten} \mbox{}\\

Die aktuelle Implementierung des A*-Algorithmus, ergibt zwar einen Pfad, welcher nicht über Hindernisse und, durch Ostacle Inflation, 
zu nah an einem Hindernis vorbeigeht, jedoch ist der resultierende Pfad für das autonome Fahrzeug dennoch nicht befahrbar. 
Das liegt daran, dass die jeweilige aktuelle Ausrichtung des Fahrzeugs nicht beachtet wird und dadurch ein Pfad resultiert, 
bei welchem das Fahrzeug sich auf der Stelle drehen müsste. 
Da dies nicht möglich ist, muss eine Optimierung des Algorithmus vorgenommen werden. 
Diese Optimierung kann gemacht werden, indem neben den \(x\) und \(y\) Koordinaten noch eine weitere Dimension hinzugefügt wird, 
welche die aktuelle Rotation enthält, die das Fahrzeug an diesem Punkt hätte. 
Außerdem werden nicht alle umliegenden Nachbarn zu der Liste hinzugefügt, sondern Punkte vor und hinter dem Fahrzeug, 
welche mit dem maximalen Lenkwinkel erreicht werden könnten. Da dies unendlich viele Punkte sind, 
sollten hier die Punkte ohne Lenkung und mit voller Lenkung nach rechts bzw. links gewählt werden. 
Für die Nachbarn wird anschließend die Rotation berechnet, die das Fahrzeug hätte, 
wenn es zu diesem Punkt gefahren wäre \cite{restricted_pathfindng_2017}. 
Aus zeitlichen Gründen konnte dieses Konzept nicht funktionieren und effizient umgesetzt werden 
[\href{https://github.com/Jundy0/Studienarbeit/blob/advanced-pathfinding/src/core/src/evasionAdvancedAStar.cpp}{src/core/src/evasionAdvancedAStar.cpp (tree advanced-pathfinding)}]. 

\paragraph{Ansteuerung des autonomen Fahrzeugs} \mbox{}\\

Nachdem ein valider Pfad erstellt wurde, muss dieser mit Fahrzeug abgefahren wurde. 
Dafür kann das \textit{VehicleActuator} Interface benutzt werden, mit welchen das echte bzw. simulierte Fahrtzeug gesteuert werden kann. 
Mit diesem kann zunächst ein Wert für die Lenkung bzw. den Motor gesetzt werden, welcher signalisiert, 
wie stark der Motor oder die Lenkung angesteuert werden soll. 
Anschließend wird in der \textit{update} Methode des \textit{SelfdrivingVehicle} die \textit{update} Methode des \textit{VehicleActuator} aufgerufen, 
in welcher anhand der gesetzten Werte, die Spannung oder das PWM Signal an den Pins setzt. 
In der Simulation wird hier lediglich der Punkt bzw. die Rotation des simulierten Fahrzeugs verändert 
[\href{https://github.com/Jundy0/Studienarbeit/blob/main/src/core/include/vehicleActuator.h}{src/core/include/vehicleActuator.h}]. 
Da über den aktuellen Ausweichalgorithmus, ein Pfad berechnet wird, welcher keinem natürlichen Fahrtverhalten entspricht, 
ist die Konvertierung von berechnetem Pfad zu Ansteuerungsanweisung noch nicht vorhanden. 

\newpage
