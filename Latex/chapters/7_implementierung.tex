\section{Implementierung}

In dem folgenden Kapitel wird beschrieben, wie die Theorie aus den vorherigen Kapiteln in der Implementierung umgesetzt wurden und die Implementierung an sich beschrieben. Dabei wird zunächst der oberflächliche Aufbau der Implementierung erklärt und dann auf die einzelnen Bestandteile der Implementierung eingegangen. Zur Verdeutlichung werden außerdem noch Besonderheiten aus der erstellten Implementierung aufgezeigt und deren Umsetzung beschrieben. 

\subsection{Aufbau der Implementierung}

Der gesamte Aufbau der Implementierung ist in drei Projekte aufgeteilt: Core, Simulation und LiDAR. Das Core-Projekt ist eine Library, welche keine ausführbare Datei und lediglich die Implementierungen der Algorithmen bzw. die Logik für das Steuern und Ausweichen des Fahrzeugs enthält. Das Simulation-Projekt dient für die Simulation des autonomen Fahrzeugs und zum Testen der implementierten Algorithmen. Das LiDAR-Projekt enthält den Code, welcher auf das eigentliche Fahrzeug, bzw. den Raspberry Pi des Fahrzeugs, geladen und auf diesem ausgeführt wird. Das Simulation- und LiDAR-Projekt benutzen das Core-Projekt, um die Logik zur Steuerung des autonomen Fahrzeugs auszuführen und verwenden dazu Schnittstellen in Form von Implementierungen mehrerer Interfaces, wodurch unter anderem das Fahrzeug gesteuert und LiDAR Daten ausgelesen werden können. Nachfolgend werden die einzelnen Projekte und die Schnittstellen, in Form der Interfaces, zwischen den Projekten näher beschrieben. 

\subsubsection{Core-Projekt}

Wie bereits beschrieben enthält das Core-Projekt die Implementierungen der verwendeten Algorithmen und die Logik zum Steuern des Fahrzeugs. Damit die Algorithmen auf Daten von Sensoren, sowie die Steuerung des Autos zuzugreifen kann, werden Interfaces als Schnittstellen verwendet, welche die Funktionen für die Algorithmen bereitstellen. Diese Interfaces werden im Core-Projekt lediglich definiert und nicht implementiert. Die Implementierung der Interfaces erfolgt in den Simulation- und LiDAR-Projekt. Dort können die Interfaces so implementiert werden, dass durch Verwendung des Interfaces die richtige Aktion im jeweiligen Projekt ausgeführt wird. So stellt z. B. das Interface zum Auslesen der LiDAR Daten im Simulations-Projekt Daten, welche den aktuellen Stand der Simulation widerspiegeln, und im LiDAR-Projekt Daten, welche über die RPLiDAR SDK aus dem verbauten LiDAR ausgelesen wurden, zurückgegeben werden. Damit die Implementierungen der Interfaces an das Core-Projekt übergeben werden können und abhängig von der aktuellen Verwendung des Core-Projektes die richtige Implementierung verwendet wird, wird das Dependency Injection Design Pattern angewandt. Dieses Design Pattern besagt, dass Abhängigkeiten, wie z. B. die Logik für das Auslesen der LiDAR Daten, ausgelagert und über festgelegt Schnittstellen von einem Injector zur Verfügung gestellt werden. In diesem Projekt werden die Schnittstellen in Form der Implementierungen der Interfaces bei der Initialisierung der Klasse, welche die Abhängigkeiten, also die Interfaces, benutzt von dem aufrufendem Code übergeben, welcher in diesem Fall als Injector fungiert \cite{dependencyInjection}. 

Neben den Interfaces für das Auslesen der LiDAR Daten und der Steuerung des Fahrzeugs befinden sich außerdem noch den Ausweichalgorithmus und den SLAM-Algorithmus, welcher ebenfalls durch Interfaces abstrahiert sind, damit auch diese einfach ausgetauscht werden können. Die gesamte Logik für das autonome Fahrzeug ist in der Klasse \dq SelfdrivingVehicle\dq\space gebündelt. Diese besitzt eine Methode \dq update\dq, welche in der Hauptschleife des jeweiligen Programms aufgerufen wird. Der Ablauf in der \dq update\dq\space Methode, ist:

\begin{enumerate}[leftmargin=*]

    \item \textbf{Auslesen der LiDAR Daten}

    Hier wird über das LiDAR Interface die aktuellen Daten des LiDARs ausgelesen und für die nächsten Schritte gespeichert.

    \item \textbf{Ausführen des SLAM-Algorithmus} 
    
    Hierfür werden die ausgelesenen LiDAR Daten, sowie die Odometrie Daten übergeben. Je nach Implementierung der Interfaces sind die Odometrie Daten Leer, da sie nicht vorhanden sind, oder werden nicht für die Ausführung des SLAM-Algorithmus verwendet. 

    \item \textbf{Ausführen des Ausweichalgorithmus}
    
    Dem Algorithmus wird die Karte, die Position und die Rotation des Fahrzeugs übergeben, welche im Schritt davor durch den SLAM-Algorithmus berechnet wurde. Daraus wird mit einem zuvor definierten Ziel ein Pfad berechnet, welcher um die erkannten Hindernisse fährt. Mit dieser wird dann berechnet, wie der Motor und die Lenkung gesetzt werden, damit das Fahrzeug auf diesem Pfad fährt. 

    \item \textbf{Updaten der Motor- und Lenksteuerung} 

    In diesem Schritt werden mit den Werten aus dem vorherigen Schritt die Steuerung für den Motor und die Lenkung geändert. Dies wird über das Interface zur Motor- und Lenksteuerung gemacht. 

\end{enumerate}

Da die gesamte Ausführung alle Schritte nicht jeden Durchlauf der Schleife nicht Nötig und mit der vorhandenen Hardware nicht effizient wäre, ist zusätzlich noch eine Überprüfung der Zeit, seit der letzten Ausführung, vorhanden. Die Zeitdifferenz, nach welcher die nächste Ausführung startet, kann variabel gesetzt werden, sollte aber zwischen 500 und 1000 ms betragen [/src/core/src/selfdrivingVehicle.cpp]. 

\subsubsection{Simulation-Projekt}

Die Simulation dient dazu, die Logik aus dem Core-Projekt zu testen, ohne dass das eigentliche Fahrzeug benötigt wird. Damit das Fahrzeug sowohl gesteuert als auch gesehen werden kann, was durch die Algorithmen berechnet wurde. Deshalb wurde die Simulation in zwei Fenster aufgeteilt, welche bei Ausführung des Programms zusammen geöffnet werden. Das erst Fenster, nachfolgend Control Window genannt, dient zur Steuerung des Fahrzeugs und der Simulation im allgemein. Es stellt in Bezug auf das autonome Fahrzeug die Realität dar. Das zweite Fenster zeigt die Ergebnisse der Algorithmen. Dabei wird die Karte, die Position und Rotation des Fahrzeugs auf der Karte, sowie der berechnete Pfad mit dem aktuellen Ziel angezeigt. In Bezug auf das autonome Fahrzeug stellt dieses Fenster die Sicht des Fahrzeugs dar. Nachfolgend wird die Funktionen und die Logik der beiden Fenster genauer erläutert. Außerdem wird noch SFML vorgestellt, was für die Umsetzung der grafischen Oberflächliche benutzt wurde. 

\paragraph{SFML} \mbox{} \\

SFML (Simple and Fast Multimedia Library) ist eine Library, mit welcher grafische Anwendungen erstellt werden können. Die Anwendung können dabei auf den gängigen Plattformen, wie Windows, Linux und MacOS laufen und können in unterschiedlichen Programmiersprachen erstellt werden, darunter auch C++ \cite{sfml}. 

\paragraph{Control Window} \mbox{} \\

In dem Control Window kann das simulierte Fahrzeug direkt über die \dq WASD\dq-Tasten gesteuert werden. Außerdem können weitere Hindernisse platziert und das Ziel geändert werden, zu welchem das Fahrzeug fahren soll. 

TODO Timo

[/src/simulation/src/controlWindow.cpp]. 

\paragraph{Visualize Window} \mbox{} \\

TODO Timo

[/src/simulation/src/visualizeWindow.cpp]. 

\subsubsection{LiDAR-Projekt}

TODO Time

\subsection{Erläuterung der Implementierung}

Nachdem nun der Aufbau der Implementierung erläutert wurde, werden nun einige Besonderheiten aus der Implementierung näher erläutert. 

\subsubsection{Simulation des LiDARs}

Ein Teil der Simulation des autonomen Fahrzeugs ist die Simulation des LiDARs. Hierfür wird die Methode des Raycasting verwendet, bei welchem Strahlen, also Vektoren, von dem Zentrum des LiDARs in alle Richtungen ausgestrahlt werden und die Schnittpunkte mit den Hindernissen gespeichert werden. Umgesetzt wurde dies, indem der gesamte Umfang des LiDARs in 360 bzw. 720 Winkel unterteilt wurde, welche die Rays darstellen, und dann für jeden Ray die Schnittpunkte für alle Hindernisse berechnet wurden. Zusätzliche wurde auch der Rahmen des Fensters als hinzugefügt, da dieser die Wände des Raums und somit auch ein Hindernis darstellt. Da alle Hindernisse, eingeschlossen Fensterrahmen, Rechtecke sind, müssen also lediglich die Schnittpunkte zwischen einem Rechteck und einem Ray, also einer Geraden, berechnet werden. Dies kann weiter unterteilt werden in den Schnittpunkt zweier Geraden, da ein Rechteck aus vier Geraden besteht. Für diesen Zweck wurde eine Funktion erstellt, welche für diesen Fall einen Schnittpunkt berechnet, falls dieser existiert. In dieser wird zunächst überprüft, ob die beiden Geraden überhaupt einen Schnittpunkt besitzen oder ob diese parallel sind. Dies kann einfach über das Berechnen des Kreuzproduktes gemacht werden. Ist das Ergebnis des Kreuzproduktes gleich \(0\), sind die beiden Geraden parallel und es gibt keinen Schnittpunkt. Als Grundlage für eigentliche Berechnung des Schnittpunktes wurde die Formel zur Berechnung des Schnittpunktes zweier Geraden benutzt, in welcher lediglich zwei Geraden in Parameterform \((Stuetzvektor + Parameter * Richtungsvektor)\) gleichgestellt werden. Da für die Berechnung der Schnittpunkte angenommen wird, dass der LiDAR bei \((0, 0)\) liegt, kann dieser in der Gleichung weggelassen werden. 

\[
s * 
\begin{pmatrix}
    rayDirection_x \\ 
    rayDirection_y
\end{pmatrix}
= 
\begin{pmatrix}
    v1_x \\ 
    v1_y
\end{pmatrix}
+ t * 
\begin{pmatrix}
    v12_x \\ 
    v12_y
\end{pmatrix}
\]

Aus dieser Formel wird ein lineares Gleichungssystem gemacht, welches anschließend nach dem Parameter \(t\) aufgelöst wird.

\[
t = \frac{v1_x * rayDirection_x - v1_y * rayDirection_y}{v12_x * rayDirection_x - v12_y * rayDirection_y} 
\]

Durch Einsetzen in eine der beiden Gleichungen des LGS kann auch der Parameter \(s\) berechnet werden. 

\[
s = \frac{v1_x + t * v12_x}{rayDirection_x};
s = \frac{v1_y + t * v12_y}{rayDirection_y}
\]

Nun muss überprüft werden, ob der Punkt nicht nur auf beiden Geraden, sondern auch zwischen den beiden Punkten des Rechtecks, sowie vor dem LiDAR liegt. Dafür wird geschaut, ob der Wert von \(t\) (Parameter für die Kante des Rechtecks) zwischen \(0\) und \(1\) liegt. Außerdem wird geschaut, ob der Wert von \(s\) (Parameter für den Ray) größer oder gleich \(0\) ist. Sollte beides gegeben sein, wird der Schnittpunkt zur Liste aller Schnittpunkte des aktuellen Rays hinzugefügt [src/simulation/src/intersection.cpp]. 

\begin{lstlisting}[caption={Berechnung des Schnittpunktes zweier Geraden},label={lst:schnittpunkt_zweier_geraden},language={C++}]
bool intersects(const sf::Vector2f &rayOrigin, const sf::Vector2f &rayDirection, const sf::Vector2f &p1, const sf::Vector2f &p2, std::vector<sf::Vector2f> &intersectionPoints)
{
    sf::Vector2f v1 = p1 - rayOrigin;
    sf::Vector2f v2 = p2 - rayOrigin;
    sf::Vector2f v12 = v2 - v1;

    const float cross = crossProduct(rayDirection, v12);

    if (cross == 0)
    {
        return false; // ray and edge are parallel
    }

    const float t = crossProduct(v1, rayDirection) / cross; // Parameter for Edge
    const float s = rayDirection.x != 0 ? ((v1.x + t * v12.x) / rayDirection.x) : ((v1.y + t * v12.y) / rayDirection.y); // Parameter for Ray

    if (t >= 0 && t <= 1 && s >= 0) // Is between points and in positive direction of Ray
    {
        const sf::Vector2f intersectionPoint = v1 + rayOrigin + t * v12;
        intersectionPoints.push_back(intersectionPoint);

        return true;
    }

    return false;
}
\end{lstlisting}

Nachdem alle Schnittpunkte berechnet wurden, muss anschließend der Schnittpunkt bestimmt werden, der am nächsten am LiDAR, also dem Stützvektor des Rays, ist. Dafür wird die Liste mit den Schnittpunkten nach dem Abstand zum LiDAR sortiert, indem der quadrierte Abstand zwischen Schnittpunkt und LiDAR berechnet. Der Vorteil des quadrierten Abstandes gegenüber dem normalen Abstand ist, dass die Reihenfolge einzelner Punkte gleich bleibt, aber die Berechnung über die Wurzel vermieden wird, welche vergleichsweise aufwendig ist \cite{stackexchangeAreThereAny2012}. Nach der Sortierung kann dann das erste Element der Liste als Schnittpunkt für diesen Ray verwendet werden. Da aktuell nur die Koordinaten des Schnittpunktes bekannt sind, ein LiDAR aber nur den Winkel und die Entfernung eines Punktes kennt, müssen für die Simulation diese Werte noch berechnet werden [src/simulation/src/intersection.cpp, src/simulation/src/lidarSensorSim.cpp]. 

\subsubsection{Implementierung des LiDARs mit der RPLIDAR SDK}

Wie bereits in der Technologie-Entscheidung beschrieben, wird für die Ansteuerung des LiDARs die RPLIDAR SDK verwendet. Zusätzlich wird die Library pigpio beutzt, um die GPIO Pins des Raspberry Pis zu verwenden. Dies wird benötigt, da der Motor des LiDAR über ein PWM-Signal gesteuert wird [src/lidar/src/a1lidarSensor.cpp]. 

TODO wie funktioniert auslesen, warum komische Werte

\begin{lstlisting}[caption={Auslesen der LiDAR Daten},label={lst:auslesen_lidar},language={C++}]
void A1LidarSensor::getScanData(lidar_point_t *data, size_t count)
{
    rplidar_response_measurement_node_hq_t scanData[count];
    u_result res = drv->grabScanDataHq(scanData, count);

    if (res == RESULT_OK)
    {
        printf("Grabbed scan data\n");
    }
    else
    {
        printf("Failed to grab scan data\n");
        printf("Error code: %d\n", res);
        return;
    }

    for (int i = 0; i < count; i++)
    {
        const double angle = scanData[i].angle_z_q14 * (90.f / 16384.f);
        const double distance = scanData[i].dist_mm_q2 / 4000.0f;
        data[i].radius = distance;
        data[i].angle = angle;
        data[i].x = distance * cos(angle);
        data[i].y = distance * sin(angle);
        data[i].quality = scanData[i].quality;
        data[i].valid = scanData[i].quality > 7;
    }
}
\end{lstlisting}

\subsubsection{Umsetzung des Ausweichalgorithmus}

% Obstacle infaltion

% A* vs. A* mit dimension für drehung

TODO Timo

\subsubsection{Umsetzung von SLAM}

TODO Justin

\newpage
